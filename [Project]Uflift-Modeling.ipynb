{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys,time,random,warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "from sklearn.model_selection import train_test_split, KFold,StratifiedKFold\n",
    "import utils\n",
    "\n",
    "# Causalml\n",
    "import causalml\n",
    "print(causalml.__version__)\n",
    "\n",
    "from causalml.dataset import synthetic_data\n",
    "from causalml.metrics.visualize import *\n",
    "from causalml.propensity import calibrate\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('causalml')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Benchmark\n",
    "from xgboost import XGBRegressor\n",
    "from causalml.inference.meta import XGBTRegressor, MLPTRegressor, LRSRegressor\n",
    "from causalml.inference.meta import BaseSRegressor, BaseTRegressor, TMLELearner\n",
    "\n",
    "# Causal net\n",
    "import causal_nets\n",
    "from causal_nets import causal_net_estimate\n",
    "\n",
    "# Dragonnet\n",
    "from causalml.inference.nn import DragonNet\n",
    "\n",
    "# CEVAE \n",
    "from causalml.inference.nn import CEVAE\n",
    "#import torch\n",
    "from causalml.propensity import ElasticNetPropensityModel\n",
    "from causalml.metrics import *\n",
    "from causalml.dataset import simulate_hidden_confounder\n",
    "\n",
    "# Tuning\n",
    "#from bayes_opt import BayesianOptimization #Cannot work\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Self-defined functions and codes that are omitted because of long running time\n",
    "# Some configuration of the plots we will create later\n",
    "%matplotlib inline  \n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "\n",
    "# APA_M1_data is an open repository for the store of the results or functions that we used in this assignment\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/Aubreyldy/APA_M1_data/main/Hillstrom.csv'\n",
    "test = pd.read_csv(url) \n",
    "\n",
    "test.head()\n",
    "\n",
    "# Query some properties of the known data\n",
    "\n",
    "print('Dimensionality of the known data is {}'.format(test.shape))  # .shape returns a tupel\n",
    "print('The known data set has {} cases.'.format(test.shape[0]))     # we can also index the elements of that tupel\n",
    "print('The total number of elements is {}.'.format(test.size))\n",
    "\n",
    "# Conversion as target\n",
    "df = test.drop(['history_segment','visit','spend'], axis=1)\n",
    "\n",
    "# One-hot-encoding except for \"segment\"\n",
    "df = pd.get_dummies(df, columns = ['zip_code','channel'],\n",
    "                     drop_first=True, dtype=np.int64)\n",
    "\n",
    "# conversion in different segments\n",
    "df.groupby(['conversion','segment']).size().unstack(fill_value=0)\n",
    "\n",
    "np.random.randn(1000, 3)\n",
    "\n",
    "df[['conversion','segment']]\n",
    "\n",
    "#df.loc[((df['segment'] == 'Mens E-Mail')\n",
    "df[['conversion','segment']].hist(by='segment',bins = 2,legend=True, stacked=True)\n",
    "\n",
    "labels = ['G1', 'G2', 'G3', 'G4', 'G5']\n",
    "men_means = [20, 35, 30, 35, 27]\n",
    "women_means = [25, 32, 34, 20, 25]\n",
    "men_std = [2, 3, 4, 1, 2]\n",
    "women_std = [3, 5, 2, 3, 3]\n",
    "width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(labels, men_means, width, yerr=men_std, label='Men')\n",
    "ax.bar(labels, women_means, width, yerr=women_std, bottom=men_means,\n",
    "       label='Women')\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Scores by group and gender')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Mix the two pairwise dataset\n",
    "df.loc[((df['segment'] == 'Mens E-Mail') | (df['segment'] == 'Womens E-Mail')),['treatment']]=1\n",
    "df.loc[(df['segment'] == 'No E-Mail'),['treatment']]=0\n",
    "\n",
    "# Before we examine sub-groups, it is meaningful to have a look at the overall ratio of treatment or not\n",
    "df['treatment'].value_counts()\n",
    "\n",
    "df.groupby(['conversion','treatment']).size().unstack(fill_value=0)\n",
    "\n",
    "#split customers into 4 groups\n",
    "df.loc[(df['treatment']==0) & (df['conversion']==1), 'response'] = 'CR'\n",
    "df.loc[(df['treatment']==0) & (df['conversion']==0), 'response'] = 'CN'\n",
    "df.loc[(df['treatment']==1) & (df['conversion']==1), 'response'] = 'TR'\n",
    "df.loc[(df['treatment']==1) & (df['conversion']==0), 'response'] = 'TN'\n",
    "df['response'] = df['response'].astype('str')\n",
    "\n",
    "pie = df.response.value_counts(normalize=True)\n",
    "print(pie)\n",
    "\n",
    "plt.rcParams['font.size'] = 10.0\n",
    "plt.pie(pie, \n",
    "        autopct='%.2f%%', \n",
    "        labels=('TN','CN','TR','CR'),\n",
    "        pctdistance=0.6,\n",
    "        explode=(0,0,0.65,0.2),\n",
    "        radius=1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,10))  # enlarge the figure\n",
    "\n",
    "# We create one histogram for each numeric variable and illustrate how to set the number of bins\n",
    "df.hist(bins=20);\n",
    "plt.tight_layout()\n",
    "\n",
    "corr= df.corr()\n",
    "f,ax = plt.subplots(figsize=(18, 15))\n",
    "sns.heatmap(corr ,annot=True,linewidth=.5,fmt='1f');\n",
    "\n",
    "# Save the space and speed up modeling\n",
    "df['recency'] = df['recency'].astype('float64')\n",
    "df['mens'] = df['mens'].astype('float64')\n",
    "df['womens'] = df['womens'].astype('float64')\n",
    "df['newbie'] = df['newbie'].astype('float64') \n",
    "df['conversion'] = df['conversion'].astype('float64')\n",
    "df['zip_code_Surburban'] = df['zip_code_Surburban'].astype('float64')\n",
    "df['zip_code_Urban'] = df['zip_code_Urban'].astype('float64')\n",
    "df['channel_Phone'] = df['channel_Phone'].astype('float64')\n",
    "df['channel_Web'] = df['channel_Web'].astype('float64')\n",
    "df['treatment'] = df['treatment'].astype('float64')\n",
    "\n",
    "# Extract target variable and feature matrix \n",
    "X = df.drop(['conversion','segment','treatment','response'], axis=1)\n",
    "Y = df[['conversion']]\n",
    "T = df[['treatment']]\n",
    "\n",
    "# Creating training and test dataset\n",
    "X_train, X_test, T_train, T_test, Y_train, Y_test = train_test_split(\n",
    "    X, T, Y, test_size=0.2, random_state=42) # copy from causal net\n",
    "### replace name\n",
    "print(X_train.shape, X_test.shape, T_train.shape, T_test.shape, Y_train.shape, Y_test.shape)\n",
    "\n",
    "df.info()\n",
    "\n",
    "# T-learner\n",
    "learner_t = BaseTRegressor(learner=XGBRegressor(learning_rate = 0.0004))\n",
    "learner_t.fit(X=X_train.values, treatment=np.ravel(T_train.values), y=np.ravel(Y_train.values))\n",
    "cate_t = learner_t.predict(X=X_test.values, treatment=np.ravel(T_test.values), y=np.ravel(Y_test.values)).flatten()\n",
    "print(cate_t)\n",
    "\n",
    "# S-learner\n",
    "learner_s = BaseSRegressor(learner=XGBRegressor(learning_rate = 0.0004))\n",
    "learner_s.fit(X=X_train.values, treatment=np.ravel(T_train.values), y=np.ravel(Y_train.values))\n",
    "cate_s = learner_s.predict(X=X_test.values, treatment=np.ravel(T_test.values), y=np.ravel(Y_test.values)).flatten()\n",
    "print(cate_s)\n",
    "\n",
    "# Getting causal estimates\n",
    "tau_pred, mu0_pred, prob_t_pred, psi_0, psi_1, history, history_ps = causal_net_estimate(\n",
    "    [X_train, T_train, Y_train], [X_train, T_train, Y_train], [X_test, T_test, Y_test],\n",
    "    hidden_layer_sizes=[60], dropout_rates=[0.5], batch_size=None, alpha=0.,\n",
    "    r_par=0., optimizer='Adam', learning_rate=0.0003,\n",
    "    max_epochs_without_change=30, max_nepochs=10000, seed=123, estimate_ps=False)\n",
    "\n",
    "## Train the neural network: Dragonnet Model\n",
    "# Take a little longer tima compared to Causal net\n",
    "\n",
    "dragon = DragonNet(neurons_per_layer=200, targeted_reg=True)\n",
    "\n",
    "## Fit the model\n",
    "dragon.fit(X_train, T_train, Y_train)\n",
    "\n",
    "## Predict ITE\n",
    "dragon_ite = dragon.predict(X_test, T_test, Y_test)\n",
    "#dragon_ate = dragon_ite.mean()\n",
    "\n",
    "## Reorder the columns of df by binary (Gausian distribution) and continous (Bernoulli distribution) variables \n",
    "# Reorder features with binary first and continuous after\n",
    "\n",
    "df_cevae = df[[\"mens\",\"womens\",\"newbie\",\"segment\",\"conversion\",\"zip_code_Surburban\",\"zip_code_Urban\",\"channel_Phone\",\"channel_Web\",\"treatment\",\"recency\",\"history\"]]\n",
    "df_cevae\n",
    "\n",
    "## Extract target variable and features \n",
    "X = df_cevae.drop(['conversion','segment','treatment'], axis=1)\n",
    "Y = df_cevae[['conversion']]\n",
    "T = df_cevae[['treatment']]\n",
    "\n",
    "## Transform columns (X, Y, T) into array \n",
    "T_array = T.loc[:,'treatment'].values\n",
    "T_array\n",
    "\n",
    "Y_array = Y.loc[:,'conversion'].values\n",
    "Y_array\n",
    "\n",
    "X_array = X.loc[:,[\"mens\",\"womens\",\"newbie\",\"zip_code_Surburban\",\"zip_code_Urban\",\"channel_Phone\",\"channel_Web\",\"recency\",\"history\"]].values\n",
    "X_array\n",
    "\n",
    "\n",
    "## Create training and test dataset\n",
    "itr, ite = train_test_split(np.arange(X_array.shape[0]), test_size=0.2, random_state=1)\n",
    "X_train_c, T_train_c, Y_train_c = X_array[itr], T_array[itr], Y_array[itr]\n",
    "X_test_c, T_test_c, Y_test_c = X_array[ite], T_array[ite], Y_array[ite]\n",
    "print(X_train_c.shape, X_test_c.shape, T_train_c.shape, T_test_c.shape, Y_train_c.shape, Y_test_c.shape)\n",
    "\n",
    "## Apply CEVAE MODEL - DEFAULT settings\n",
    "\n",
    "'''cevae = CEVAE(outcome_dist=\"bernoulli\", #not default     ##Outcome distribution: \"bernoulli\" , \"exponential\", \"laplace\", \"normal\", \"studentt\"\n",
    "              latent_dim=20,                          ##Dimension of latent variable\n",
    "              hidden_dim=200,                         ##Dimension of hidden layers of fully connected networks\n",
    "              num_epochs=50,                         ##Number of training epochs\n",
    "              batch_size=100,                        ##Batch size\n",
    "              learning_rate=0.001,                    ##the final learning rate will be learning_rate * learning_rate_decay\n",
    "              learning_rate_decay=0.01, \n",
    "              num_layers=3) #not 2                    ##Number of hidden layers in fully connected networks\n",
    "\n",
    "## fit \n",
    "losses = cevae.fit(X=torch.tensor(X_train_c, dtype=torch.float),\n",
    "                   treatment=torch.tensor(T_train_c, dtype=torch.float),\n",
    "                   y=torch.tensor(Y_train_c, dtype=torch.float))\n",
    "\n",
    "## predict\n",
    "cevae_ite = cevae.predict(X_test_c, T_test_c, Y_test_c)'''\n",
    "\n",
    "# Because it took one hour to get the result, we store the data from prediction and import it directly\n",
    "\n",
    "# Read the result\n",
    "data_url1 = 'https://raw.githubusercontent.com/Aubreyldy/APA_M1_data/main/cevae_loss.csv'\n",
    "data_url2 = 'https://raw.githubusercontent.com/Aubreyldy/APA_M1_data/main/cevae_ite.csv'\n",
    "\n",
    "losses = pd.read_csv(data_url1)\n",
    "cevae_ite = pd.read_csv(data_url2)\n",
    "\n",
    "cevae_ite1 = cevae_ite['cevae_ite']\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/aubrey/Documents/GitHub/APA_M1_data\")  # Change the Path\n",
    "import utils\n",
    "\n",
    "###### T-learner\n",
    "Tlearner = pd.DataFrame(columns=['lr','ate','auuc','qini'])\n",
    "\n",
    "# Time records\n",
    "start_time = time.time()\n",
    "# Meta-parameter of one-layer-nn\n",
    "\n",
    "# index for store\n",
    "s=1\n",
    "\n",
    "for h in np.linspace(0.0002,0.001,num=5):\n",
    "    Params = {'1': h}\n",
    "    ate_1, auuc_1, qini_1= cv_cn(\"T_learner\",X_train, X_test,\n",
    "                                 T_train, T_test, Y_train, Y_test,Params)\n",
    "    Tlearner.loc[s, ['lr']] = h\n",
    "    Tlearner.loc[s, ['ate']] = ate_1\n",
    "    Tlearner.loc[s, ['auuc']] = auuc_1\n",
    "    Tlearner.loc[s, ['qini']] = qini_1\n",
    "    s=s+1\n",
    "        \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "###### S-learner\n",
    "Slearner = pd.DataFrame(columns=['lr','ate','auuc','qini'])\n",
    "start_time = time.time()\n",
    "# Meta-parameter of one-layer-nn\n",
    "\n",
    "# index for store\n",
    "s=1\n",
    "\n",
    "for h in np.linspace(0.0002,0.001,num=5):\n",
    "    Params = {'1': h}\n",
    "    ate_2, auuc_2, qini_2= cv_cn(\"S_learner\",X_train, X_test,\n",
    "                                 T_train, T_test, Y_train, Y_test,Params)\n",
    "    Slearner.loc[s, ['lr']] = h\n",
    "    Slearner.loc[s, ['ate']] = ate_2\n",
    "    Slearner.loc[s, ['auuc']] = auuc_2\n",
    "    Slearner.loc[s, ['qini']] = qini_2\n",
    "    s=s+1\n",
    "        \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Read the result\n",
    "data_url = 'https://raw.githubusercontent.com/Aubreyldy/APA_M1_data/main/ATE_AUUC_hls.csv'\n",
    "ATE_AUUC_hls = pd.read_csv(data_url)\n",
    "#ATE_AUUC_hls = pd.read_csv('.../ATE_AUUC_hls.csv',) \n",
    "ATE_AUUC_hls = ATE_AUUC_hls.drop(ATE_AUUC_hls.columns[0], axis=1)\n",
    "\n",
    "# Plot Function\n",
    "utils.ourplot(range(10,41), ATE_AUUC_hls,ATE_AUUC_hls, y11='ATE_hls',y12='ATE_hls', y21='AUUC_hls',y22='AUUC_hls',\n",
    "        x='Numbers of Neurons in the Second Hidden Layer',\n",
    "        title='ATE and AUUC via Different Numbers of Neurons in the Second Hidden Layer',num=1)\n",
    "\n",
    "# Select the Hidden Layer Size with Best AUUC\n",
    "A = np.array(ATE_AUUC_hls.AUUC_hls==max(ATE_AUUC_hls.AUUC_hls))\n",
    "print(\"The maximum of AUUC is: \", max(ATE_AUUC_hls.AUUC_hls))\n",
    "print(\"The corresponding QINI is: \", ATE_AUUC_hls.QINI_hls[np.where(A)[0].item()])\n",
    "print(\"The corresponding ATE is: \", ATE_AUUC_hls.ATE_hls[np.where(A)[0].item()])\n",
    "\n",
    "# Read the result\n",
    "ATE_AUUC_dr = pd.read_csv('.../ATE_AUUC_dr.csv',) \n",
    "ATE_AUUC_dr = ATE_AUUC_dr.drop(ATE_AUUC_dr.columns[0], axis=1)\n",
    "\n",
    "utils.ourplot(np.linspace(0,0.9,num=10), ATE_AUUC_dr,ATE_AUUC_dr,y11='ATE_dr', y12='ATE_dr', y21='AUUC_dr',y22='AUUC_dr',\n",
    "        x='Droupout Rates',\n",
    "        title='ATE and AUUC via Different Droupout Rates',num=1)\n",
    "\n",
    "# Select the Dropout Rate with Best AUC\n",
    "A = np.array(ATE_AUUC_dr.AUUC_dr==max(ATE_AUUC_dr.AUUC_dr))\n",
    "print(\"The maximum of AUUC is: \", max(ATE_AUUC_dr.AUUC_dr))\n",
    "print(\"The corresponding QINI is: \", ATE_AUUC_dr.QINI_dr[np.where(A)[0].item()])\n",
    "print(\"The corresponding ATE is: \", ATE_AUUC_dr.ATE_dr[np.where(A)[0].item()])\n",
    "\n",
    "# Read the result\n",
    "ATE_AUUC_hln = pd.read_csv('.../ATE_AUUC_hln.csv',) \n",
    "ATE_AUUC_hln = ATE_AUUC_hln.drop(ATE_AUUC_hln.columns[0], axis=1)\n",
    "\n",
    "utils.ourplot(range(10,41),ATE_AUUC_hls,ATE_AUUC_hln,y11='ATE_hls',y12='ATE_hln',y21='AUUC_hls',y22='AUUC_hln',\n",
    "       x='Numbers of Neurons in the Second Hidden Layer: i',\n",
    "       title='Comparison bewteen different pools',num=2,nn=31,lab='hidden_layer_size=[30]')\n",
    "\n",
    "# Read the result\n",
    "ATE_AUUC_2 = pd.read_csv('.../ATE_AUUC_2.csv',) \n",
    "ATE_AUUC_2 = ATE_AUUC_2.drop(ATE_AUUC_2.columns[0], axis=1)\n",
    "\n",
    "ATE_AUUC_2.columns = ['hidden layer size','dropout rate','learning rate','ATE','AUUC']\n",
    "\n",
    "%matplotlib inline  \n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "sns.lineplot(data=ATE_AUUC_2, x=\"dropout rate\", y=\"AUUC\", hue=\"hidden layer size\", style=\"learning rate\")\n",
    "\n",
    "# Read the result\n",
    "dragon2 = pd.read_csv('.../dragon1.csv',) \n",
    "dragon2 = dragon2.drop(dragon2.columns[0], axis=1)\n",
    "\n",
    "# Plot\n",
    "%matplotlib inline  \n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "sns.lineplot(data=dragon2, x=\"r\", y=\"auuc\", hue=\"nrp\", style=\"lr\")\n",
    "\n",
    "# The Best Pool from Above\n",
    "dragon2[dragon2['auuc']==max(dragon2['auuc'])]\n",
    "\n",
    "alpha=0.7\n",
    "bins=30\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.hist(cate_t, alpha=alpha, bins=bins, label='T-learner')\n",
    "plt.vlines(cate_s[0], 0, plt.axes().get_ylim()[1], label='S-lbearner',\n",
    "           linestyles='dotted', colors='green', linewidth=2)\n",
    "plt.title('Distribution of CATE Predictions by Benchmark Models')\n",
    "plt.xlabel('Individual Treatment Effect (ITE/CATE)')\n",
    "plt.ylabel('# of Samples')\n",
    "_=plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "alpha=0.7\n",
    "bins=30\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(tau_pred, alpha=alpha, bins=bins, label='Causal net')\n",
    "plt.xlabel('Individual Treatment Effect (ITE/CATE)')\n",
    "plt.ylabel('# of Samples')\n",
    "_=plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist((dragon_ite[:,1]-dragon_ite[:,0]), bins=bins, alpha=alpha,  label='Dragonnet')\n",
    "plt.xlabel('Individual Treatment Effect (ITE/CATE)')\n",
    "plt.ylabel('# of Samples')\n",
    "_=plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(cevae_ite1, alpha=alpha,  label='CEVAE')\n",
    "plt.xlabel('Individual Treatment Effect (ITE/CATE)')\n",
    "plt.ylabel('# of Samples')\n",
    "_=plt.legend()\n",
    "\n",
    "plt.suptitle('Distribution of CATE Predictions by Different Methods')\n",
    "plt.tight_layout()\n",
    "\n",
    "dft = pd.DataFrame({'y': np.ravel(Y_test), 'w': np.ravel(T_test),\n",
    "                    'T-Learner': cate_t.flatten(),'S-Learner': cate_s.flatten(),\n",
    "                    'Causal net': tau_pred.flatten(),\n",
    "                   'Dragonnet': (dragon_ite[:,1]-dragon_ite[:,0]).flatten(),\n",
    "                   'CEVAE': cevae_ite1})\n",
    "plot(dft, outcome_col='y', treatment_col='w')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
