{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blueprint Design\n",
    "## Package Import and Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.feature_selection import SelectPercentile, SelectKBest, f_regression, f_classif\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, Normalizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, f1_score, plot_roc_curve, confusion_matrix, \\\n",
    "ConfusionMatrixDisplay, brier_score_loss, accuracy_score\n",
    "\n",
    "from datetime import date\n",
    "import scorecardpy as sc\n",
    "\n",
    "# Some configuration of the plots we will create later\n",
    "%matplotlib inline  \n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "import shap\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncation\n",
    "truncate_item_price = True\n",
    "truncate_delivery_days = False\n",
    "truncate_age = False\n",
    "cut_age = True\n",
    "\n",
    "# imputation\n",
    "numeric_imputer_strategy =  'median' # 'median', 'most_frequent', 'constant' 'mean'\n",
    "numeric_standard_scaler_mean = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define Methods and Wrap them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df, known_data:bool, truncate_delivery_days:bool, truncate_item_price:bool, truncate_age:bool, cut_age:bool):\n",
    "    # change object variables to datatype category\n",
    "    # change numeric variables from float64 to float32 (reduce memory consumption)\n",
    "    # change feature return to boolean (2 categories)\n",
    "    # change dates to the datetime datatype 'datetime64[ns]'\n",
    "    df = transform_columns(df, known_data)\n",
    "    \n",
    "    # via (df['delivery_date'] - df['order_date']).dt.days\n",
    "    df = add_delivery_days(df)\n",
    "\n",
    "    if truncate_delivery_days:\n",
    "        print('truncate_delivery_days')\n",
    "        # via outlier_truncation(df['delivery_days'])\n",
    "        # # Define upper/lower bound\n",
    "        # # upper = x.quantile(0.75) + factor*IQR\n",
    "        # # lower = x.quantile(0.25) - factor*IQR\n",
    "        df = remove_delivery_days_outliers(df)\n",
    "    \n",
    "    # via df['delivery_date'].apply(lambda x: False if pd.isnull(x) else True)\n",
    "    df = add_delivery_date_missing(df)\n",
    "\n",
    "    # year<2016 is all 1994, which is suspicious\n",
    "    # via df['delivery_date'].apply(lambda x: True if x.year < 2016 else False)\n",
    "    df = add_delivery_date_1994_marker(df)\n",
    "\n",
    "    # via df.loc[df['delivery_date'].dt.year < 2016,['delivery_days']] = np.nan\n",
    "    df = set_delivery_date_1994_to_nan(df)\n",
    "    \n",
    "    # via df['brand_id'].apply(lambda x: (df['brand_id'] == x).sum())\n",
    "    df = add_brand_id_count(df)\n",
    "\n",
    "    # via df['item_id'].apply(lambda x: (df['item_id'] == x).sum())\n",
    "    df = add_item_id_count(df)\n",
    "\n",
    "    # set it all to lowercase and correct some spelling error\n",
    "    # then via df['item_color'].apply(lambda x: (df['item_color'] == x).sum())\n",
    "    df = add_item_color_count(df)\n",
    "    \n",
    "    # a practical summary for retailing size:\n",
    "    # sizes_dict = {\n",
    "    #     '84': 'xxs', '104': 's', '110': 's', '116': 's', '122': 'm', '128': 'm',\n",
    "    #     '134': 'l', '140': 'l', '148': 'xl', '152': 'xl', '164': 'xxl', '170': 'xxl',\n",
    "    #     '176': 'xxxl', '18': 'xs', '19': 's', '20': 's', '21': 'm', '22': 'm', '23': 'l',\n",
    "    #     '24':  'xl', '25': 'xs', '26': 's', '27': 's', '28': 'm', '29': 'm',  '30': 'l',\n",
    "    #     '31': 'l', '32': 'xl', '33': 'xxl', '34': 'xxs', '35': 'xs', '36': 'xs', '36+': 's',\n",
    "    #     '37': 's', '37+': 's', '38': 's', '38+': 's', '39': 'm', '39+': 'm', '40': 'm',\n",
    "    #     '40+': 'm', '41': 'm', '41+' : 'm', '42': 'l', '42+': 'l', '43': 'l', '43+': 'l',\n",
    "    #     '44': 'l', '44+' : 'xl', '45' : 'xl', '45+': 'xl', '46': 'xl', '46+' : 'xl',\n",
    "    #     '47' : 'xl', '48': 'xl', '49': 'xl', '50': 'xxl', '52': 'xxl', '54': 'xxl',\n",
    "    #     '56': 'xxl', '58': 'xxl', 0: 'xxs', '1': 'xxs', '2': 'xxs', '2+': 'xxs', '3' : 'xxs',\n",
    "    #     '3+': 'xs', '4':  'xs', '4+': 'xs', '5': 'xs', '5+':'xs', '6':'s', '6+':'s',\n",
    "    #     '7':'s', '7+':'m', '8':'m', '8+':'m', '9': 'l', '9+': 'l', '10': 'l', '10+': 'xl',\n",
    "    #     '11': 'xl', '11+': 'xl', '12': 'xl', '12+': 'xxl', '13': 'xxl', '14': 'xxl',\n",
    "    #     36: 'xxs', 38: 'xs', 40: 's', 42: 'm', 44: 'l', 46: 'xl', 48: 'xxl',\n",
    "    #     '3132': 'xxs', '3332': 'xs', '3432': 'xs', '3632': 's', '3832': 'm', '3634': 'l',\n",
    "    #     '3834': 'xl', '4032': 'xl', '4034': 'xxl', '4232': 'xxxl', '80': 'xs', '85': 's',\n",
    "    #     '90': 'm', '95': 'l', '100': 'xl', '105': 'xxl'\n",
    "    # }\n",
    "    df = convert_item_sizes(df)\n",
    "\n",
    "    if truncate_item_price:\n",
    "        print('truncate_price_size')\n",
    "        # via outlier_truncation(df['item_price'])\n",
    "        df = truncate_item_price_outliers(df)\n",
    "\n",
    "    # via df['user_dob'].apply(calculate_age)\n",
    "    # today = date.today()\n",
    "    # return today.year - born.year - ((today.month, today.day) < (born.month, born.day))\n",
    "    df = add_age(df)\n",
    "\n",
    "    if truncate_age:\n",
    "        print('truncate_age')\n",
    "        # via outlier_truncation(df['age'])\n",
    "        df = truncate_age_outliers(df)\n",
    "    if cut_age:\n",
    "        print('cut_age')\n",
    "        # via df.loc[df['age'] > 95,'age'] = np.nan\n",
    "        df = cut_age_outliers(df)\n",
    "    \n",
    "    # via df['user_dob'].apply(lambda x: False if pd.isnull(x) else True)\n",
    "    df = add_dob_missing(df)\n",
    "\n",
    "    # via df['been_member_for'] = (df['order_date']-df['user_reg_date'] ).dt.days\n",
    "    df = add_been_member_for(df)\n",
    "\n",
    "    # labels = ['fresh_member', 'new_member', 'member', 'old_member']\n",
    "    # cut_bins = [-5, 150, 300, 450, 1000]\n",
    "    # via df['member'] = pd.cut(df['been_member_for'], bins=cut_bins, labels=labels)\n",
    "    df = add_member_category(df)\n",
    "\n",
    "    print(df.info())\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA and Data Preparation\n",
    "### Load Data and Take a First Glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_known = pd.read_csv('.../BADS_WS2021_known.csv', index_col='order_item_id') \n",
    "df_known.head()\n",
    "# Query some properties of the data\n",
    "print('Dimensionality of the data is {}'.format(df_known.shape))  # .shape returns a tupel\n",
    "print('The data set has {} cases.'.format(df_known.shape[0]))     # we can also index the elements of that tupel\n",
    "print('The total number of elements is {}.'.format(df_known.size))\n",
    "df_known.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion After Comparison of Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_columns(df_known, known_data:bool):\n",
    "    # change object variables to datatype category\n",
    "    df_known['item_size'] = df_known['item_size'].astype('category')\n",
    "    df_known['item_color'] = df_known['item_color'].astype('category')\n",
    "    df_known['user_title'] = df_known['user_title'].astype('category')\n",
    "    df_known['user_state'] = df_known['user_state'].astype('category')\n",
    "    # change all numeric variables from float64 to float32 to reduce memory consumption\n",
    "    df_known['item_price'] = df_known['item_price'].astype(np.float32)\n",
    "    df_known['item_price'] = df_known['item_price'].apply(lambda x:(\"%.2f\" % round(x, 2)))\n",
    "    df_known['item_price'] = df_known['item_price'].astype(np.float32)\n",
    "    df_known['brand_id'] = df_known['brand_id'].astype(np.int32)\n",
    "    df_known['user_id'] = df_known['user_id'].astype(np.int32)\n",
    "    df_known['item_id'] = df_known['item_id'].astype(np.int32)\n",
    "    if known_data:\n",
    "        # since the feature return has only two values, we convert it to boolean\n",
    "        df_known['return'] = df_known['return'].astype('bool')\n",
    "    # transform all dates to the datetime datatype\n",
    "    df_known['order_date'] = df_known['order_date'].astype('datetime64[ns]')\n",
    "    df_known['delivery_date'] = df_known['delivery_date'].astype('datetime64[ns]')\n",
    "    df_known['user_dob'] = df_known['user_dob'].astype('datetime64[ns]')\n",
    "    df_known['user_reg_date'] = df_known['user_reg_date'].astype('datetime64[ns]')\n",
    "    return df_known\n",
    "\n",
    "df = transform_columns(df, known_data=True)\n",
    "df.info()\n",
    "\n",
    "# <class 'pandas.core.frame.DataFrame'>\n",
    "# Int64Index: 100000 entries, 1 to 100000\n",
    "# Data columns (total 13 columns):\n",
    "#  #   Column         Non-Null Count   Dtype         \n",
    "# ---  ------         --------------   -----         \n",
    "#  0   order_date     100000 non-null  datetime64[ns]\n",
    "#  1   delivery_date  90682 non-null   datetime64[ns]\n",
    "#  2   item_id        100000 non-null  int32         \n",
    "#  3   item_size      100000 non-null  category      \n",
    "#  4   item_color     100000 non-null  category      \n",
    "#  5   brand_id       100000 non-null  int32         \n",
    "#  6   item_price     100000 non-null  float32       \n",
    "#  7   user_id        100000 non-null  int32         \n",
    "#  8   user_title     100000 non-null  category      \n",
    "#  9   user_dob       91275 non-null   datetime64[ns]\n",
    "#  10  user_state     100000 non-null  category      \n",
    "#  11  user_reg_date  100000 non-null  datetime64[ns]\n",
    "#  12  return         100000 non-null  bool          \n",
    "# dtypes: bool(1), category(4), datetime64[ns](4), float32(1), int32(3)\n",
    "# memory usage: 5.8 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Data Description During Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe feature\n",
    "df['order_date'].describe()\n",
    "\n",
    "# Count \"return\" according to \"delivery_date\"\n",
    "## Bar plot\n",
    "df.loc[df[\"delivery_date\"].isna(), ['return']]['return'].value_counts()#.plot(kind=\"bar\")\n",
    "\n",
    "# Head data of multiple features\n",
    "df[['delivery_date', 'delivery_date_missing', 'return']].head()\n",
    "\n",
    "# Cross table\n",
    "pd.crosstab(df['user_title'], df[\"return\"], normalize='index')\n",
    "\n",
    "# Correlation between features\n",
    "df[['delivery_date_missing', 'return']].corr()\n",
    "# Heatmap\n",
    "sns.heatmap(df_corr.corr(),\n",
    "            annot=True);\n",
    "# Boxplot\n",
    "df.boxplot(column='delivery_days')\n",
    "# Violin plot\n",
    "for col in ['order_age','order_age_combine','order_deliver','reg_order']:\n",
    "    plt.figure()\n",
    "    sns.violinplot(x='user_title', y=col, hue='return',\n",
    "                   split=True, inner=\"quart\",\n",
    "                   data= df1, subplots=True)\n",
    "                   \n",
    "# Select specific data according to specific criterion\n",
    "df[df['delivery_days'] < 0]\n",
    "\n",
    "# Categorical feature distribution\n",
    "# Exluding data type float leaves us with the target variable and both categorical variables\n",
    "plt.figure(figsize=(12,6))\n",
    "for i, col in enumerate(df.select_dtypes('category').columns):\n",
    "    plt.figure(i)\n",
    "    a = sns.countplot(x=col, data=df)\n",
    "    a.set_xticklabels(a.get_xticklabels(), rotation=50, ha=\"right\", fontsize=11)  \n",
    "\n",
    "# Stack count plot\n",
    "for i, col in enumerate(['order_date_weekend','delivery_date_weekend',\n",
    "                         'user_reg_date_weekend']):\n",
    "    plt.figure(i)\n",
    "    df1.groupby(['return', col]).size().reset_index().pivot(\n",
    "        columns='return', index=col, values=0).plot(kind='bar',\n",
    "                                                    stacked=True)\n",
    "\n",
    "\n",
    "# WoE (Weight of Evidence, just as an example, it will not be done in this section)\n",
    "bins_been_member_for = sc.woebin(df, y=\"return\", x='been_member_for')\n",
    "\n",
    "sc.woebin_plot(bins_been_member_for)\n",
    "\n",
    "# Numerical feature distribution\n",
    "sns.distplot(df['item_price'])\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(18,12))  # enlarge the figure\n",
    "# We create one histogram for each numeric variable and illustrate how to set the number of bins\n",
    "df['item_price'].hist(bins=20)\n",
    "# or\n",
    "sns.distplot(df['been_member_for'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove correlated features\n",
    "df = df.drop(columns=['brand_id_frequency', 'brand_id_count']) # negative correlation with 'brand_id_woe'\n",
    "\n",
    "# Filter function\n",
    "class filter_binary_target:\n",
    "    def __init__(self, df, target):\n",
    "        self.target = target\n",
    "        self.data_head = df.head()\n",
    "\n",
    "    def auto_filter_binary_target(self):\n",
    "        print('Data must be in a clean pandas DataFrame. Categorical variables must be of data type bool or category. Continuous variables must be int64 or float64.')\n",
    "        data_no_target = df.drop(columns=self.target)\n",
    "        columns = ['Data Type', 'Metric', 'Score']\n",
    "        index = data_no_target.columns\n",
    "        result = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "        for col in data_no_target:\n",
    "            if data_no_target.dtypes[col] == 'bool' or data_no_target.dtypes[col].name == 'category':\n",
    "                result.loc[col, 'Data Type'] = \"discrete\"\n",
    "                result.loc[col, 'Metric'] = \"IV\"\n",
    "                result.loc[col, 'Score'] = self.IV_binary_target(feature=col)\n",
    "\n",
    "            if data_no_target.dtypes[col] == 'int64' or data_no_target.dtypes[col] == 'float64':\n",
    "                result.loc[col, 'Data Type'] = \"continuous\"\n",
    "                result.loc[col, 'Metric'] = \"Fisher\"\n",
    "                result.loc[col, 'Score'] = self.fisher_binary_target(feature=col)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def IV_binary_target(self, feature):  # same code as used above\n",
    "        data = pd.DataFrame()\n",
    "    \n",
    "        data['Count'] = df[feature].value_counts()\n",
    "        data['Bad'] = df.groupby([feature])[self.target].sum()\n",
    "        data['Good'] = data['Count'] - data['Bad']\n",
    "    \n",
    "        data[\"Distribution Bad\"] = data[\"Bad\"] / data[\"Bad\"].sum()\n",
    "        data[\"Distribution Good\"] = data[\"Good\"] / data[\"Good\"].sum()\n",
    "    \n",
    "        data['WOE'] = np.log(data[\"Distribution Good\"] / data[\"Distribution Bad\"])\n",
    "        data.replace({\"WOE\": {np.inf: 0, -np.inf: 0}})\n",
    "\n",
    "        data[\"IV\"] = data[\"WOE\"] * (data[\"Distribution Good\"] - data[\"Distribution Bad\"])\n",
    "\n",
    "        iv = data[\"IV\"].sum()\n",
    "\n",
    "        return iv\n",
    "\n",
    "    def fisher_binary_target(self, feature):\n",
    "        mu_0 = df.groupby(df[self.target])[feature].mean()[0]\n",
    "        mu_1 = df.groupby(df[self.target])[feature].mean()[1]\n",
    "        var_0 = df.groupby(df[self.target])[feature].var()[0]\n",
    "        var_1 = df.groupby(df[self.target])[feature].var()[1]\n",
    "\n",
    "        num = abs(mu_0 - mu_1)\n",
    "        den = (var_0 + var_1) ** 0.5\n",
    "        score = num/den\n",
    "    \n",
    "        return score\n",
    "\n",
    "    def pearson(self, feature):  # since our target is binary, we actually don't need this. However, if you would like to expand this class, you can use this code\n",
    "        mean_feature = df[feature].mean()\n",
    "        mean_target = df[self.target].mean()\n",
    "        num = ((df[feature] - mean_feature)*(df[self.target] - mean_target)).sum()\n",
    "        den = (((df[feature] - mean_feature)**2).sum() * ((df[self.target] - mean_target)**2).sum()) ** .5\n",
    "        rho = num/den\n",
    "        return rho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline construction\n",
    "### Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only specified columns\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.columns]\n",
    "        \n",
    "# Drop only specified columns, it will be used after completely preprocessing the data before traing model\n",
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, drop_list):\n",
    "        self.drop_list = drop_list\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        df = X.drop(list(self.drop_list), axis=1)\n",
    "        \n",
    "        # Rename\n",
    "        df.columns = X_var_labels\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Weight-of-Evidence\n",
    "class WoETransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Calculate the WoE\"\"\"\n",
    "    def __init__(self, target, feature):\n",
    "        self.target = target\n",
    "        self.feature = feature\n",
    " \n",
    "    def fit(self, df, y):\n",
    "        \n",
    "        #self.feature = df.drop(['return'],axis = 1).columns.values[0]\n",
    "        df = pd.concat([df,y],axis=1)\n",
    "        #count_values\n",
    "        data = pd.DataFrame()\n",
    "        data['Count'] = df[self.feature].value_counts()               # Count instances of each category, create row for each\n",
    "        data['Bad'] = df.groupby([self.feature])[self.target].sum()   # Count y=1 instances of that category\n",
    "        data['Good'] = data['Count'] - data['Bad']                    # Count y=0 instances of that category\n",
    "        data = data.sort_values(by=[\"Count\"], ascending=False)\n",
    "    \n",
    "        try:\n",
    "            assert data[\"Bad\"].sum() != 0                               # Check that there are y=1 instances in sample\n",
    "            assert data[\"Good\"].sum() != 0                              # Check that there are y=0 instances in sample\n",
    "            assert np.isin(df[self.target].unique(), [0, 1]).all()      # Check that target includes only 0,1 or True,False\n",
    "        except:\n",
    "          print(\"Error: Target must include 2 binary classes.\")\n",
    "          raise     \n",
    "        \n",
    "        #distribution\n",
    "        data[\"WOE_adj\"] = np.log( \n",
    "            ((data[\"Count\"] - data[\"Bad\"] + 0.5) / (data[\"Count\"].sum() - data[\"Bad\"].sum())) / \n",
    "            ((data[\"Bad\"] + 0.5) / data[\"Bad\"].sum())\n",
    "            )\n",
    "        data.replace({\"WOE_adj\": {np.inf: 0, -np.inf: 0}})\n",
    "        self.data = data.sort_values(by=[\"Count\"], ascending=False)\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df.loc[:, self.feature] = df.loc[:, self.feature].map(self.data[\"WOE_adj\"])\n",
    "        return df\n",
    "\n",
    "\n",
    "filter = filter_binary_target(df=df, target=\"return\")\n",
    "\n",
    "filter.auto_filter_binary_target()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessor combinition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woe_pipe = Pipeline([('step_1',WoETransformer(feature = 'item_id', target = 'return')),\n",
    "                      ('step_2',WoETransformer(feature = 'brand_id', target = 'return')),\n",
    "                      ('step_3',WoETransformer(feature = 'user_id', target = 'return')),\n",
    "                      ('step_4',WoETransformer(feature = 'item_size', target = 'return')),\n",
    "                      ('step_5',WoETransformer(feature = 'item_color', target = 'return')),\n",
    "                      ('step_6',WoETransformer(feature = 'user_state', target = 'return'))])\n",
    "\n",
    "\n",
    "std_pipe = Pipeline([('selector', ColumnSelector(['order_age_combine','order_deliver',\n",
    "                                                 'reg_order', 'item_price'])),\n",
    "                     ('scaler', StandardScaler())])\n",
    "\n",
    "\n",
    "preprocessor = FeatureUnion(transformer_list=[('WoE', woe_pipe),\n",
    "                                              ('Std', std_pipe)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final DataFrame (just an example) and train-test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target variable and feature matrix \n",
    "X = df.drop(['return'], axis=1)\n",
    "y = df[['return']]\n",
    "\n",
    "# Change the dependent variable from float32 to bool\n",
    "y = y.astype('bool')\n",
    "\n",
    "# Split data into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.loc[0:99999,:], y.loc[0:99999,:], test_size=0.3, random_state=888)\n",
    "\n",
    "# Combine X_train and y_train into dataframe\n",
    "Xy_train = pd.concat([X_train, y_train], axis = 1)\n",
    "Xy_test = pd.concat([X_test, y_test], axis = 1)\n",
    "\n",
    "print(\"Remember the shape of our data: \")\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, Xy_train.shape, Xy_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(max_iter=1000, C=1.0, fit_intercept=True)\n",
    "lr_param_grid = {\n",
    "    'preprocessor__num__imputer__metric': ['median', 'mean'],\n",
    "    'preprocessor__num__scaler__method': ['std', 'minmax'],\n",
    "    'select__percentile': [5, 10, 25, 50],\n",
    "    'classifier__C': [0.1, 1.0, 10, 100],\n",
    "    'classifier__fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "clf_lr = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        # Select features according to the k highest F-score.\n",
    "        ('select', SelectKBest(score_func=f_regression, k='all')), \n",
    "        ('classifier', model_lr)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# gs_lr = GridSearchCV(estimator=clf_xgb, param_grid=lr_param_grid, scoring='roc_auc', cv=5, verbose=0)\n",
    "# gs_lr.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "clf_lr.fit(X_train, y_train)\n",
    "\n",
    "# ROC Curve\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
